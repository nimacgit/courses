{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"direction:rtl;line-height:300%;\">\n",
    "\t<font face=\"XB Zar\" size=5>\n",
    "\t\t<div align=center>\n",
    "\t\t\t<font face=\"IranNastaliq\" size=30>\n",
    "\t\t\t\t<p></p>\n",
    "\t\t\t\t<p></p>\n",
    "به نام خدا\n",
    "\t\t\t\t<p></p>\n",
    "\t\t\t</font>\n",
    "\t\t\t<font color=#FF7500>\n",
    "دانشگاه صنعتی شریف - دانشکده مهندسی کامپیوتر\n",
    "            </font>\n",
    "\t\t\t<p></p>\n",
    "\t\t\t<font color=blue>\n",
    "هوش مصنوعی\n",
    "            </font>\n",
    "\t\t\t<br />\n",
    "\t\t\t<br />\n",
    "پاییز ۱۳۹۹\n",
    "\t\t</div>\n",
    "\t\t<div align=center>\n",
    "\t\t    <font color=#FF7500 size=6>\n",
    "\t\t\t    <br />\n",
    "تمرین ششم، بخش دوم\n",
    "                سوال عملی\n",
    "            \t<br/>\n",
    "\t\t\t</font>\n",
    "        </div>\n",
    "\t</font>\n",
    "    <hr/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "pdf-title"
    ]
   },
   "source": [
    "<div dir = \"rtl\" style=\"direction:rtl;line-height:200%;\">\n",
    "\t<font face=\"XB Zar\" size=3>\n",
    "در این تمرین با پیمودن گام‌های مشخص شده یک شبکه عصبی دولایه با معماری ساده پیاده‌سازی می‌کنید. در ابتدا با اجرا کردن قطعه کد تنظیمات اولیه انجام می‌شوند. در هنگام انجام تمرین و مراجعه به فایل کد در آن قسمت‌های TODO گذاشته شده که شما باید آن قسمت‌ها را پر کنید و توضیحاتی علاوه بر توضیحات این فایل jupyter نیز در آنجا داده شده.<br>\n",
    "        این تمرین برگرفته از تمرین‌های درس cs231n می‌باشد.\n",
    "\t</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [],
   "source": [
    "# A bit of setup\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from neural_net import TwoLayerNet\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "def rel_error(x, y):\n",
    "    \"\"\" returns relative error \"\"\"\n",
    "    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "pdf-ignore"
    ]
   },
   "source": [
    "<div dir = \"rtl\" style=\"direction:rtl;line-height:200%;\">\n",
    "\t<font face=\"XB Zar\" size=3>\n",
    "        کلاس \n",
    "   TwoLayerNet در فایل neural_net.py متعلق به مدلی که قصد طراحی آن را دارید می‌باشد که کد‌های خود را در این فایل پیاده سازی می‌کنید.\n",
    "    پارامتر‌های شبکه در self.params ذخیره می‌شوند. کد زیر به تست کردن کد شما در حین پیاده‌سازی کمک می‌کند بنابراین بدون ایجاد تغییر در بلوک زیر آن را اجرا کنید.\n",
    "        <br>\n",
    "        معماری این شبکه به صورت زیر می‌باشد:<br>\n",
    "        <ul>\n",
    "            <li>ورودی دارای D ویژگی می‌باشد سپس لایه‌ی مخفی دارای H عدد پرسپترون می‌باشد. این شبکه در نهایت مقدار c مقدار خروجی می‌دهد که تعداد کلاس‌هایی که می‌خواهیم شناسایی کنیم می‌باشد. دقت کنید که این سه مقدار به شبکه به عنوان ورودی داده می‌شود و بر اساس آن شبکه ساخته می‌شود.</li>\n",
    "            <li>\n",
    "                در این شبکه بعد از لایه‌ی اول از تابع فعال‌سازی relu استفاده می‌کنیم و در نهایت برای محاسبه کردن هزینه‌ی شبکه عصبی با توجه به اینکه c مقدار خروجی داریم از تابع هزینه‌ی softmax استفاده می‌کنیم که در ادامه با آن آشنا خواهید شد.\n",
    "                $$input 🡒 hidden\\, layer 🡒 relu 🡒 output\\, layer 🡒 softmax$$\n",
    "            </li>\n",
    "        </ul>\n",
    "\t</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [],
   "source": [
    "# Create a small net and some toy data to check your implementations.\n",
    "# Note that we set the random seed for repeatable experiments.\n",
    "\n",
    "input_size = 4\n",
    "hidden_size = 10\n",
    "num_classes = 3\n",
    "num_inputs = 5\n",
    "\n",
    "def init_toy_model():\n",
    "    np.random.seed(0)\n",
    "    return TwoLayerNet(input_size, hidden_size, num_classes, std=1e-1)\n",
    "\n",
    "def init_toy_data():\n",
    "    np.random.seed(1)\n",
    "    X = 10 * np.random.randn(num_inputs, input_size)\n",
    "    y = np.array([0, 1, 2, 2, 1])\n",
    "    return X, y\n",
    "\n",
    "net = init_toy_model()\n",
    "X, y = init_toy_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir = \"rtl\" style=\"direction:rtl;line-height:200%;\">\n",
    "\t<font face=\"XB Zar\" size=3>\n",
    "    <h3>محاسبه‌ی امتیازات</h3>\n",
    "        در این مرحله باید قسمتی از تابع loss در کلاس TwoLayerNet را پیاده کنید(تا جایی که امتیازهای مربوط به هر کلاس بدست می‌آیند) در مرحله‌ی بعد امتیازات محاسبه شده توسط شما به تابع softmax loss داده می‌شود تا هزینه‌ی این شبکه بدست آید.\n",
    "        <br>\n",
    "        پس از اجرای بلوک زیر مقدار بدست آمده توسط کد شما و مقدار اصلی مقایسه می‌شوند و تفاوت آن‌ها نمایش داده می‌شود. این مقدار باید بسیار کوچک باشد (در حد $ 10^{-8}$)\n",
    "    <br> به نکات کلی زیر نیز توجه کنید:\n",
    "        <ul>\n",
    "            <li>\n",
    "                w1 وزن‌های مربوط به لایه‌ی اول می‌باشد که ماتریس از با شکل (D,H) می‌باشد که D تعداد ویژگی‌های ورودی‌ها و H تعداد پرسپترون‌های لایه‌ی مخفی می‌باشد و مقدار آن در self.params[\"w1\"] ذخیره شده است.\n",
    "            </li>\n",
    "            <li>\n",
    "                b1 برای بایاس‌های مربوط به لایه‌ی اول می‌باشد که دارای شکل (,H) می‌باشد که مقدار آن در self.params[\"b1\"] دخیره شده است.\n",
    "            </li>\n",
    "            <li>\n",
    "                w2 ماتریس وزن‌ها برای لایه‌ی خروجی می‌باشد که دارای شکل (H,C) می‌باشد که مقدار آن در self.params[\"w2\"] ذخیره شده است.\n",
    "            </li>\n",
    "            <li>\n",
    "                b2 ماتریس مربوط به بایاس‌های لایه‌ی خروجی می‌باشد که دارای شکل (,C) می‌باشد که مقدار آن در self.params[\"b2\"] ذخیره شده است.\n",
    "            </li>\n",
    "            <li>\n",
    "                H,C,D ورودی‌های مدل می‌باشند.\n",
    "            </li>\n",
    "        </ul>\n",
    "        نکات زیر نیز درباره ورودی و خروجی تابع مهم می‌باشند:\n",
    "        <ul>\n",
    "            <li>\n",
    "                ورودی x داده‌های ورودی به شبکه می‌باشند که یک ماتریس با ابعاد (N,D) می‌باشند.\n",
    "            </li>\n",
    "            <li>\n",
    "                ورودی y که یک ماتریس با ابعاد (,N) می‌باشد نشان دهنده‌ی کلاس درست مربوط به ورودی‌های x می‌باشد. این ورودی می‌تواند به تابع داده شود یا نشود در صورتی که داده نشود این تابع فقط امتیازات را محاسبه می‌کند و خروجی می‌دهد و در صورتی که داده شود با استفاده از آن‌ها و امتیازات بدست آمده مقادیر گرادیان و هزینه را نیز محاسبه می‌کند و آن‌ها را خروجی می‌دهد.\n",
    "            </li>\n",
    "        </ul>\n",
    "    </font>\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = net.loss(X)\n",
    "print('Your scores:')\n",
    "print(scores)\n",
    "print()\n",
    "print('correct scores:')\n",
    "correct_scores = np.asarray([\n",
    "  [-0.81233741, -1.27654624, -0.70335995],\n",
    "  [-0.17129677, -1.18803311, -0.47310444],\n",
    "  [-0.51590475, -1.01354314, -0.8504215 ],\n",
    "  [-0.15419291, -0.48629638, -0.52901952],\n",
    "  [-0.00618733, -0.12435261, -0.15226949]])\n",
    "print(correct_scores)\n",
    "print()\n",
    "\n",
    "# The difference should be very small. We get < 1e-7\n",
    "print('Difference between your scores and correct scores:')\n",
    "print(np.sum(np.abs(scores - correct_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir = \"rtl\" style=\"direction:rtl;line-height:200%;\">\n",
    "\t<font face=\"XB Zar\" size=3>\n",
    "    <h3>محاسبه‌ی هزینه</h3>\n",
    "    در این مرحله هزینه شبکه عصبی با استفاده از امتیازاتی که در مرحله‌ی قبل بدست آوردید توسط تابع هزینه‌ی softmax محاسبه می‌شود (همچنان در همان تابع loss که در مرحله قبل در حال پر کردن قسمتی از آن بودید این کار را انجام می‌دهید). در این مرحله شما صرفا باید به محاسبات انجام شده مقدار مربوط به regularization را اضافه کنید.(مقدار loss که محاسبه شده است را باید آپدیت کنید) تابع softmax به صورت زیر است:(مقدار تفاوت باید حدودا $10^{-13}$ باشد) \n",
    "    </font>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$L_i=-\\log{(\\frac{e^{f_{y_i}}}{\\sum_{k}{e^{f_{yk}}}})}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, _ = net.loss(X, y, reg=0.05)\n",
    "correct_loss = 1.30378789133\n",
    "\n",
    "# should be very small, we get < 1e-12\n",
    "print('Difference between your loss and correct loss:')\n",
    "print(np.sum(np.abs(loss - correct_loss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir = \"rtl\" style=\"direction:rtl;line-height:200%;\">\n",
    "\t<font face=\"XB Zar\" size=3>\n",
    "    <h3>محاسبه‌ی گرادیان</h3>\n",
    "    در این بخش به محاسبه‌ی گرادیان ها می‌پردازیم. این گرادیان‌ها را برای بروزرسانی کردن w1, w2, b1, b2 می‌باشد تا هزینه‌ی شبکه‌ی عصبی کاهش یابد و خطای آن کمتر شود. برای انجام محاسبات دقت کنید که گرادیان قسمت softmax محاسبه شده و در dscores قرار گرفته و شما باید با استفاده از آن و با کمک back-propagation گرادیان‌های W1,W2,b1,b2 را محاسبه کنید.<br>\n",
    "        برای پیاده‌سازی این قسمت در ادامه‌ی همان تابع loss این کار را انجام دهید و در قسمت مربوطه پیاده‌سازی کنید. دقت شود که مقادیر بدست آمده را باید در یک dictionary برگردانید که key همان مقدار می‌باشد و value باید ماتریس گرادیان را قرار دهید دقت شود که ابعاد ماتریس گرادیان باید مانند خود ماتریس وزن یا بایاس‌ها باشد. همچنین در محاسبات دخالت دادن L2 Regularization را فراموش نکنید.(خطای محاسبات باید حدودا برابر با $10^{-9}$ باشد) \n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gradient_check import eval_numerical_gradient\n",
    "\n",
    "# Use numeric gradient checking to check your implementation of the backward pass.\n",
    "# If your implementation is correct, the difference between the numeric and\n",
    "# analytic gradients should be less than 1e-8 for each of W1, W2, b1, and b2.\n",
    "\n",
    "loss, grads = net.loss(X, y, reg=0.05)\n",
    "\n",
    "# these should all be less than 1e-8 or so\n",
    "for param_name in grads:\n",
    "    f = lambda W: net.loss(X, y, reg=0.05)[0]\n",
    "    param_grad_num = eval_numerical_gradient(f, net.params[param_name], verbose=False)\n",
    "    print('%s max relative error: %e' % (param_name, rel_error(param_grad_num, grads[param_name])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir = \"rtl\" style=\"direction:rtl;line-height:200%;\">\n",
    "\t<font face=\"XB Zar\" size=3>\n",
    "    <h3>آموزش شبکه عصبی</h3>\n",
    "        در این قسمت برای آموزش شبکه عصبی از stochastic gradient descent استفاده می‌کنیم. برای اینکار باید تابع train در مدل را پیاده‌سازی کنید. علاوه بر تابع train باید تابع predict را نیز پیاده کنید زیرا در حین آموزش برای محاسبه‌ی دقت از این تابع استفاده می‌شود. پس از پیاده‌سازی بلوک زیر را اجرا کنید مقدار هزینه باید کمتر از 0.02 شود.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "final_training_loss"
   },
   "outputs": [],
   "source": [
    "net = init_toy_model()\n",
    "stats = net.train(X, y, X, y,\n",
    "            learning_rate=1e-1, reg=5e-6,\n",
    "            num_iters=100, verbose=False)\n",
    "\n",
    "print('Final training loss: ', stats['loss_history'][-1])\n",
    "\n",
    "# plot the loss history\n",
    "plt.plot(stats['loss_history'])\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('training loss')\n",
    "plt.title('Training Loss history')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir = \"rtl\" style=\"direction:rtl;line-height:200%;\">\n",
    "\t<font face=\"XB Zar\" size=3>\n",
    "    <h3>خواندن داده‌ها</h3>\n",
    "    حال که پیاده‌سازی قسمت‌های بالا تمام شد با اجرای بلوک زیر داده‌ها که در پوشه datasets قرار داده شده خوانده می‌شوند و در مراحل بعد بر روی آن‌ها شبکه را آموزش می‌دهیم.\n",
    "</font>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [],
   "source": [
    "from data_utils import load_CIFAR10\n",
    "\n",
    "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000):\n",
    "    \"\"\"\n",
    "    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
    "    it for the two-layer neural net classifier. These are the same steps as\n",
    "    we used for the SVM, but condensed to a single function.  \n",
    "    \"\"\"\n",
    "    # Load the raw CIFAR-10 data\n",
    "    cifar10_dir = 'datasets/cifar-10-batches-py'\n",
    "    \n",
    "    # Cleaning up variables to prevent loading data multiple times (which may cause memory issue)\n",
    "    try:\n",
    "        del X_train, y_train\n",
    "        del X_test, y_test\n",
    "        print('Clear previously loaded data.')\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "        \n",
    "    # Subsample the data\n",
    "    mask = list(range(num_training, num_training + num_validation))\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = list(range(num_training))\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = list(range(num_test))\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "\n",
    "    # Normalize the data: subtract the mean image\n",
    "    mean_image = np.mean(X_train, axis=0)\n",
    "    X_train -= mean_image\n",
    "    X_val -= mean_image\n",
    "    X_test -= mean_image\n",
    "\n",
    "    # Reshape data to rows\n",
    "    X_train = X_train.reshape(num_training, -1)\n",
    "    X_val = X_val.reshape(num_validation, -1)\n",
    "    X_test = X_test.reshape(num_test, -1)\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "\n",
    "# Invoke the above function to get our data.\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = get_CIFAR10_data()\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir = \"rtl\" style=\"direction:rtl;line-height:200%;\">\n",
    "\t<font face=\"XB Zar\" size=3>\n",
    "    <h3>یادگیری</h3>\n",
    "     با اجرای بلوک زیر یک شبکه با پارامتر‌های مشخص شده آموزش داده می‌شود. دقت این شبکه باید نسبتا پایین باشد.(حدود ۴۸ درصد) دلیل این می‌باشد که این شبکه برای این مجموعه داده آسان می‌باشد (دقت این شبکه به نسبت شبکه‌های آسان‌تر مانند knn یا شبکه‌ی یک لایه‌ی عادی بالاتر می‌باشد. برای اینکه این دقت افزایش باید از شبکه‌های پیچیده‌تر که از سطح این درس بالاتر می‌باشند استفاده کرد (مانند شبکه‌های چند لایه و یا CNNها) که پیشنهاد می‌شود برای مطالعه بیشتر به آن‌ها مراجعه کنید.<br>\n",
    "        به نکات زیر توجه درباره‌ی این تابع train توجه کنید:\n",
    "        <ul>\n",
    "            <li>X: یک آرایه‌ی numpy با ابعاد (N,D) که داده‌های قسمت training را فراهم می‌کند.</li>\n",
    "            <li>Y: یک آرایه‌ی numpy با ابعاد (,N) که جواب‌های مربوط به قسمت training را فراهم می‌کند که سطر iام آن به این معناست که کلاس سطر iام X چه می‌باشد.</li>\n",
    "            <li>X_val: مانند X فقط برای قسمت validation استفاده می‌شود.</li>\n",
    "            <li>Y_val: مانند Y فقط باری قسمت validation استفاده می‌شود.</li>\n",
    "            <li>learning_rate: پارامتر مربوط سرعت آموزش می‌باشد.</li>\n",
    "            <li>learning_rate_decay: بعد از هر epoch پارامتر مربوط به سرعت یادگیری در این مقدار ضرب می‌شود و کوچکتر می‌شود.</li>\n",
    "            <li>reg: پارامتر مربوط به regularization می‌باشد.</li>\n",
    "            <li>num_iters: تعداد iteration هایی که برای عمل بهینه‌سازی مورد استفاده قرار می‌گیرد.</li>\n",
    "            <li>verbos: مقدار بولین که تصمیم می‌گیرد در حین اجرای این تابع اطلاعت چاپ شوند یا نه.</li>\n",
    "        </ul>\n",
    "        همانطور که گفته شد در این قسمت باید تابع predict را نیز پیاده‌سازی کنید به نکات زیر در مورد آن دقت کنید:\n",
    "        <ul>\n",
    "            <li> در این تابع باید با استفاده از وزن‌های یادگرفته شده پیشبینی‌های خود را درباره‌ی ورودی‌ها برگردانید.</li>\n",
    "            <li>X: ورودی با ابعاد (N,D) می‌باشد.</li>\n",
    "            <li>y_pred: خروجی تابع می‌باشد که پیشبینی‌های مربوط به ورودی X می‌باشد و سایز آن برابر با (,N) می‌باشد.</li>\n",
    "        </ul>\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "code"
    ]
   },
   "outputs": [],
   "source": [
    "input_size = 32 * 32 * 3\n",
    "hidden_size = 200\n",
    "num_classes = 10\n",
    "net = TwoLayerNet(input_size, hidden_size, num_classes)\n",
    "\n",
    "# Train the network\n",
    "stats = net.train(X_train, y_train, X_val, y_val,\n",
    "            num_iters=1000, batch_size=200,\n",
    "            learning_rate=1.0e-03, learning_rate_decay=0.95,\n",
    "            reg=0.15, verbose=True)\n",
    "\n",
    "# Predict on the validation set\n",
    "val_acc = (net.predict(X_val) == y_val).mean()\n",
    "print('Validation accuracy: ', val_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
